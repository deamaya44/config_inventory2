# Compresi√≥n de Archivos Hist√≥ricos

Este documento explica c√≥mo funciona la compresi√≥n autom√°tica de archivos hist√≥ricos en el inventario de AWS Config.

## üóúÔ∏è ¬øQu√© se comprime?

**Archivos comprimidos autom√°ticamente:**
- ‚úÖ CSV hist√≥rico: `current-inventory_YYYYMMDD_HHMMSS.csv.gz`
- ‚úÖ CSV Excel hist√≥rico: `current-inventory-excel_YYYYMMDD_HHMMSS.csv.gz`
- ‚úÖ Resumen JSON hist√≥rico: `current-summary_YYYYMMDD_HHMMSS.json.gz`

**Archivos SIN comprimir (para datasources):**
- ‚ùå `current-inventory.csv` (usado por QuickSight)
- ‚ùå `current-inventory-excel.csv` (acceso directo)
- ‚ùå `current-summary.json` (consulta r√°pida)

## üíæ Ahorro de Espacio

### Compresi√≥n t√≠pica para archivos CSV/JSON:
- **CSV**: 70-85% de reducci√≥n
- **JSON**: 80-90% de reducci√≥n
- **Ejemplo**: Archivo de 1.4 MB ‚Üí ~200-400 KB

### Beneficios:
- **Reducci√≥n de costos S3** significativa
- **Transferencias m√°s r√°pidas**
- **Archivos hist√≥ricos organizados** y eficientes
- **Retenci√≥n a largo plazo** m√°s econ√≥mica

## üìÅ Estructura de Archivos

```
s3://bucket/aws-config-inventory/
‚îú‚îÄ‚îÄ current/                           # Sin comprimir (datasources)
‚îÇ   ‚îú‚îÄ‚îÄ current-inventory.csv
‚îÇ   ‚îú‚îÄ‚îÄ current-inventory-excel.csv
‚îÇ   ‚îî‚îÄ‚îÄ current-summary.json
‚îî‚îÄ‚îÄ historical/                        # Comprimidos (archivo)
    ‚îú‚îÄ‚îÄ current-inventory_20251125_031442.csv.gz
    ‚îú‚îÄ‚îÄ current-inventory-excel_20251125_031442.csv.gz
    ‚îú‚îÄ‚îÄ current-summary_20251125_031442.json.gz
    ‚îú‚îÄ‚îÄ current-inventory_20251125_032502.csv.gz
    ‚îî‚îÄ‚îÄ ...
```

## üõ†Ô∏è C√≥mo Descomprimir Archivos

### M√©todo 1: Script Python incluido

```bash
# Listar archivos comprimidos disponibles
python scripts/decompress_historical.py list --bucket aws-config-inventory-028139915738

# Descomprimir un archivo espec√≠fico
python scripts/decompress_historical.py decompress \
  --bucket aws-config-inventory-028139915738 \
  --key aws-config-inventory/historical/current-inventory_20251125_031442.csv.gz \
  --output inventory_20251125.csv
```

### M√©todo 2: AWS CLI + gzip

```bash
# Descargar y descomprimir en un solo comando
aws s3 cp s3://bucket/path/file.csv.gz - | gunzip > file.csv

# O descargar primero, luego descomprimir
aws s3 cp s3://bucket/path/file.csv.gz ./
gunzip file.csv.gz
```

### M√©todo 3: Python program√°tico

```python
import boto3
import gzip

def download_and_decompress(bucket, key, local_file):
    s3 = boto3.client('s3')
    
    # Descargar archivo comprimido
    response = s3.get_object(Bucket=bucket, Key=key)
    compressed_data = response['Body'].read()
    
    # Descomprimir y guardar
    decompressed_data = gzip.decompress(compressed_data)
    with open(local_file, 'wb') as f:
        f.write(decompressed_data)

# Uso
download_and_decompress(
    'aws-config-inventory-028139915738',
    'aws-config-inventory/historical/current-inventory_20251125_031442.csv.gz',
    'inventory.csv'
)
```

## üìä Metadatos de Compresi√≥n

Cada archivo comprimido incluye metadatos:

```json
{
  "original-size": "1458968",
  "compression": "gzip",
  "format": "excel-friendly",
  "description": "CSV optimizado para Excel con Account IDs formateados (comprimido)"
}
```

## üîç Verificaci√≥n de Compresi√≥n

Para verificar que la compresi√≥n funciona:

```bash
# Ver archivos en S3 con tama√±os
aws s3 ls s3://bucket/aws-config-inventory/ --recursive --human-readable

# Ejemplo de output:
# 2025-11-24 22:14:43   1.4 MiB current/current-inventory.csv
# 2025-11-24 22:14:44 400.2 KiB historical/current-inventory_20251125_031442.csv.gz
```

## ‚öôÔ∏è Configuraci√≥n T√©cnica

### En el c√≥digo Python:
```python
def compress_string_to_gzip(data_string):
    """Comprime un string a formato gzip"""
    data_bytes = data_string.encode('utf-8')
    compressed_buffer = BytesIO()
    
    with gzip.GzipFile(fileobj=compressed_buffer, mode='wb') as gz_file:
        gz_file.write(data_bytes)
    
    compressed_buffer.seek(0)
    return compressed_buffer
```

### Metadatos S3:
- `ContentType`: `application/gzip`
- `ContentEncoding`: `gzip`
- `Metadata`: Informaci√≥n de tama√±o original y compresi√≥n

## üö® Consideraciones Importantes

### ‚úÖ Ventajas:
- **Ahorro significativo de espacio** (70-90%)
- **Reducci√≥n de costos S3** a largo plazo
- **Transferencias m√°s r√°pidas**
- **Archivos actuales sin comprimir** para acceso directo

### ‚ö†Ô∏è Consideraciones:
- **Archivos hist√≥ricos requieren descompresi√≥n** para acceso
- **Ligero overhead de CPU** durante compresi√≥n (m√≠nimo)
- **Herramientas deben soportar gzip** (mayor√≠a lo hace)

## üìà An√°lisis de Costos

### Ejemplo mensual (30 ejecuciones cada 10 min = 4,320 ejecuciones):
- **Sin compresi√≥n**: 4,320 √ó 1.4 MB = ~6 GB/mes
- **Con compresi√≥n**: 4,320 √ó 0.3 MB = ~1.3 GB/mes
- **Ahorro**: ~78% en costos de almacenamiento S3

### Para retenci√≥n de 1 a√±o:
- **Sin compresi√≥n**: ~72 GB
- **Con compresi√≥n**: ~15.6 GB
- **Ahorro anual**: Significativo en cuentas AWS con muchos recursos

## üîß Troubleshooting

### Error: "No such file or directory"
```bash
# Verificar que el archivo existe
aws s3 ls s3://bucket/path/file.gz

# Verificar permisos
aws sts get-caller-identity
```

### Error de descompresi√≥n:
```bash
# Verificar integridad del archivo
aws s3api head-object --bucket bucket --key path/file.gz
```

### Archivos muy grandes:
```bash
# Para archivos grandes, usar streaming
aws s3 cp s3://bucket/file.gz - | gunzip | head -n 100
```